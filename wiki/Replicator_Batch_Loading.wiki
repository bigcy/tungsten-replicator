#summary Theory and Practice of Batch Loading using Tungsten

= Introduction =

Tungsten Replicator normally applies SQL changes to slaves by constructing SQL statements and executing in the exact order that transactions appear in the Tungsten History Log (THL).  This works well for OLTP databases like MySQL, PostgreSQL, Oracle, and MongoDB.  However, it is a poor approach for data warehouses. 

Data warehouse products like Vertica or GreenPlum load very slowly through JDBC interfaces (50 times slower or even more compared to MySQL).  Instead, we would really like to use a batch loader.  It turns out that both Vertica and Greenplum support the PostgreSQL COPY command, which allows data to be loaded quickly from CSV.  

Batch loading is good for OLTP databases, too.  MySQL's LOAD DATA INFILE can operate 20x faster than SQL inserts in some cases.  This makes it desirable for loading row updates quickly.  

The Tungsten [http://code.google.com/p/tungsten-replicator/source/browse/trunk/replicator/src/java/com/continuent/tungsten/replicator/applier/batch/BatchApplier.java BatchApplier] class is the first implementation of batch loading for Tungsten Replicator.  The rest of this file describes how it works. 

= Theory of Operation =

Java class BatchApplier loads data into the slave DBMS using CVS files and LOAD DATA INFILE.  The first implementation works on MySQL but is being extended to work on PostgreSQL-like databases.  Here is the basic algorithm.  

While executing within a commit block, we write incoming transactions into open CSV files written by class CsvWriter.   There is a maximum of one insert CSV and one delete CSV file per database table. 
  * Inserts go into an insert CSV file for their table.   (The insert file contains the column values.) 
  * Deletes go into a delete CSV file for their table.  (The delete file contains only keys.) 
  * Updates put keys into the delete file and the updated values into the insert file.  
  * Statements are ignored.   If you want DDL you need to put it in yourself. 

When we commit the block, the load algorithm works as follows. 
  # Close each delete CSV and load it into a temp table.  Delete any row in the corresponding real table that matches a key in the temp table. 
  # Close each insert CSV and load it into the real table. 

So much for the algorithm.  If there are no delete operations this approach is equivalent to serial execution of the SQL operations but theoretically much faster because it avoids a lot of network round-trips and as well as overhead of the SQL itself.

= Batch Applier Setup =

Here is how to set up on MySQL.  This will be made easier in future 
when the batch loader is added to the installations. 

1. Enable row replication on the MySQL master using {{{set global binlog_format=row}}} or by updating my.cnf.  

2. Create a normal master/slave pair.  Here's a typical installation command.  The options for validation checks are necessary due a problem in the tungsten-installer.  

{{{
tools/tungsten-installer --master-slave -a \
  --master-host=logos1  \
  --datasource-user=tungsten  \
  --datasource-password=secret  \
  --service-name=percona \
  --home-directory=/opt/rhodges4 \
  --cluster-hosts=logos1,logos2 \
  --channels=1 \
  --buffer-size=100 \
  --mysql-use-bytes-for-string=false \
  --skip-validation-check=MySQLConfigFileCheck \
  --skip-validation-check=MySQLApplierServerIDCheck \
  --svc-parallelization-type=disk --start-and-report 
}}}

Once the slave is set up, go to that host and manually edit the replication service properties file (static-percona.properties in my example) as follows: 

1. Find the slave pipeline definition and set the following property.  The pkey and colnames filters are required so you get the proper names for columns and reduce the keys to only the columns that are actually primary keys.   Note:  you don't want to eliminate extra update columns as you are going to insert the full row. 

{{{
replicator.stage.q-to-dbms.filters=pkey,colnames,bidiSlave
}}}

2. Find the applier section and comment/out whatever is there.  Replace it with the following: 

{{{
replicator.applier.dbms=com.continuent.tungsten.replicator.applier.batch.BatchApplier
replicator.applier.dbms.url=jdbc:mysql:thin://${replicator.global.db.host}:${replicator.global.db.port}/tungsten_${service.name}?createDB=true
replicator.applier.dbms.driver=org.drizzle.jdbc.DrizzleDriver
replicator.applier.dbms.user=${replicator.global.db.user}
replicator.applier.dbms.password=${replicator.global.db.password}
replicator.applier.dbms.template=LOAD DATA INFILE '%%FILE%%' REPLACE INTO TABLE %%TABLE%% FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '"'
replicator.applier.dbms.stagingDirectory=/tmp/staging
replicator.applier.dbms.supportsReplace=true
}}}

Restart the slave replicator and batch loading is enabled.  From there on out you can start to test. 

= Testing and Operation = 

You can test BatchApplier using the new croc utility from bristlecone.  Croc is documented in bristlecone/doc/CROC.TXT.  

You can also test using any load within the capabilities described in the next section.  Remember to run DDL on both master and slave databases. 

= Capabilities =

The BatchApplier implementation has the following general capabilities.  

  * The BatchApplier should correctly apply any sequence of insert and update equivalently to the normal fully serialized operation with SQL statements.  
  * INSERT, UPDATE, and DELETE statements should be handled.  
  * The following datatypes are supported:  INT, BIGINT, DATE, TIMESTAMP, VARCHAR, DOUBLE, and FLOAT. 
  * Replicator management functions, for example the trepctl, should function exactly as with normal fully serialized replication.  
  * Configuration parameters like the global buffer size (replicator.global.buffer.size) behave exactly as before.  In fact, this value should be increased to a very large value for the last stage so that the CSV files contain as many rows as possible at commit time.
  * The replicator will clean up CSV files and temp tables after commit and whenever it goes offline. 
  
= Limitations =

There are a number of limitations to the work done so far. 

  * The applier currently only works on MySQL but is being extended for PostgreSQL-style databases.  
  * Deletes can cause problems.  For instance, issuing an INSERT followed by a DELETE on the same key value will be inverted if they execute in the same commit block.  BatchApplier will apply the DELETE first without effect, then INSERT the row.  
  * Not all datatypes are supported or even tested. 
  * Parallel replication has not been tested. 