#summary Theory and Practice of Batch Loading using Tungsten
= Contents = 
<wiki:toc max_depth="1" />
= Introduction =

Tungsten Replicator normally applies SQL changes to slaves by constructing SQL statements and executing in the exact order that transactions appear in the Tungsten History Log (THL).  This works well for OLTP databases like MySQL, PostgreSQL, Oracle, and MongoDB.  However, it is a poor approach for data warehouses. 

Data warehouse products like Vertica or GreenPlum load very slowly through JDBC interfaces (50 times slower or even more compared to MySQL).  Instead, we would really like to use a batch loader.  It turns out that both Vertica and Greenplum support the PostgreSQL COPY command, which allows data to be loaded quickly from CSV.  

Batch loading is good for OLTP databases, too.  MySQL's LOAD DATA INFILE can operate 20x faster than SQL inserts in some cases.  This makes it desirable for loading row updates quickly.  

The Tungsten [http://code.google.com/p/tungsten-replicator/source/browse/trunk/replicator/src/java/com/continuent/tungsten/replicator/applier/batch/BatchApplier.java BatchApplier] class is the first implementation of batch loading for Tungsten Replicator.  The rest of this file describes how it works. 

= Theory of Operation =

Java class BatchApplier loads data into the slave DBMS using CVS files and LOAD DATA INFILE.  The first implementation works on MySQL and PostgreSQL databases.  Here is the basic algorithm.  

While executing within a commit block, we write incoming transactions into open CSV files written by class CsvWriter.   There is one CSV file per database table.  The following sample shows typical contents.  
{{
"84900","I","986","http://www.continent.com/software","1"
"84901","D","143",null,"2"
"84901","I","143","http://www.microsoft.com","3"
}}

Rows start with the Tungsten sequence number (seqno) followed by a transaction code "I" for insert and "D" for delete.  The last value in the row is a row ID that shows the order in which rows were written to the file.  Different update types are handled as follows: 

  * Each insert generates a single row containing all values in the row with an "I" opcode. 
  * Each delete generates a single row with the key and a "D" opcode.  Non-key fields are null. 
  * Each update results in a delete with the row key followed by an insert.  
  * Statements are ignored.   If you want DDL you need to put it in yourself. 

Tungsten writes each row update into the corresponding CSV file for the SQL.  At commit time the following steps occur.  
  # Flush and close each CSV file.  This ensures that if there is a failure the files are fully visible in storage. 
  # Execute a load script to move the data from CSV into a staging table using a command like COPY or LOAD DATA INFILE.  
  # Execute a merge script to move the data from the staging table to the underlying base table. 

Staging tables have a special format that is described in a later section.  Staging tables introduce an extra step but have the advantage that users can apply flexible logic to decide how to load data.  This has applications that go beyond data warehouse loading.  For instance, you can use staging tables to implement conflict resolution rules for multi-master replication.   

The main requirement of the load and merge scripts is that they must ensure rows load and that delete and insert operations apply in the correct order.   Tungsten includes load scripts for MySQL and Vertica that do this.  

= Batch Applier Setup =

Here is how to set up on MySQL.  This will be made easier in future 
when the batch loader is added to the installations. 

1. Enable row replication on the MySQL master using {{{set global binlog_format=row}}} or by updating my.cnf.  

2. Install using the --batch-enable=true option.  Here's a typical installation command.  The options for validation checks are necessary due a problem in the tungsten-installer.  

{{{
tools/tungsten-installer --direct -a \
  --master-host=logos1  \
  --master-user=tungsten  \
  --master-password=secret  \
  --slave-host=logos2 \
  --slave-user=tungsten  \
  --slave-password=secret  \
  --service-name=batch \
  --batch-enabled=true \
  --batch-load-template=mysql \
  --home-directory=/opt/continuent \
  --channels=1 \
  --buffer-size=1000 \
  --mysql-use-bytes-for-string=false \
  --skip-validation-check=MySQLConfigFileCheck \
  --skip-validation-check=MySQLExtractorServerIDCheck \
  --skip-validation-check=MySQLApplierServerIDCheck \
  --property=replicator.filter.pkey.addPkeyToInserts=true \
  --property=replicator.filter.pkey.addColumnsToDeletes=true \
  --svc-parallelization-type=disk --start-and-report 
}}}


2. Find the applier section and comment out or delete whatever is there.  Replace it with the following: 

{{{
# Batch applier basic configuration information. 
replicator.applier.dbms=com.continuent.tungsten.replicator.applier.batch.BatchApplier
replicator.applier.dbms.url=jdbc:mysql:thin://${replicator.global.db.host}:${replicator.global.db.port}/tungsten_${service.name}?createDB=true
replicator.applier.dbms.driver=org.drizzle.jdbc.DrizzleDriver
replicator.applier.dbms.user=${replicator.global.db.user}
replicator.applier.dbms.password=${replicator.global.db.password}

# Timezone and character set. 
replicator.applier.dbms.timezone=GMT+0:00
replicator.applier.dbms.charset=UTF-8

# Load method, COPY command template, and staging directory location. 
replicator.applier.dbms.loadMethod=direct
replicator.applier.dbms.loadBatchTemplate=LOAD DATA INFILE '%%FILE%%' REPLACE INTO TABLE %%TABLE%% CHARACTER SET utf8 FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '"'
replicator.applier.dbms.stageDirectory=/tmp/staging
replicator.applier.dbms.supportsReplace=true

# Extra parameters for loading via stage tables. 
replicator.applier.dbms.stageTablePrefix=stage_xxx
replicator.applier.dbms.stageInsertFromTemplate=INSERT INTO %%BASE_TABLE%%(%%BASE_COLUMNS%%) SELECT %%BASE_COLUMNS%% FROM %%STAGE_TABLE%%
replicator.applier.dbms.stageDeleteFromTemplate=DELETE FROM %%BASE_TABLE%% WHERE %%BASE_PKEY%% IN (SELECT %%STAGE_PKEY%% FROM %%STAGE_TABLE%%)
replicator.applier.dbms.stagePkeyColumn=id
replicator.applier.dbms.stageRowIdColumn=row_id
replicator.applier.dbms.cleanUpFiles=false
}}}

The properties that begin with "stage" are required only if you select staged as the load method.  There are no defaults for these values.  You must specify them fully or replication will fail.  

As shown in the example, SQL templates can be parameterized by values that Tungsten Replicator fills in at runtime.  The example shows all the currently supported values.  

Restart the slave replicator and batch loading is enabled.  From there on out you can start to test. 

= Transfer Table Format =

Transfer tables must follow a specific format that mimics the base table to which they apply.  There is a separate insert and delete table for each base table.  

For example, suppose we have a base table created by the following CREATE TABLE command: 

{{{
CREATE TABLE `croc_insertfloat` (
  `id` int(11) NOT NULL,
  `f_data` float DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
}}}

To make inserts on this table you must create an insert transfer table that has the same format including column names, types, and order.  Beyond that there are two differences.  First, the table should not have a primary key or you will get primary key violations if there are multiple inserts on the same row.  Second, the last column must contain a row_id column, which is used to distinguish between multiple inserts.  Here is the corresponding table matching the preceding example: 

{{{
CREATE TABLE `stage_xxx_insert_croc_insertfloat` (
  `id` int(11) NOT NULL,
  `f_data` float DEFAULT NULL,
  `row_id` int(11) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
}}}

To make deletes (or updates) on the table you must create a delete transfer table that contains only the primary key from the base table plus a row ID.  This looks like the following.  

{{{
CREATE TABLE `stage_xxx_delete_croc_insertfloat` (
  `id` int(11) DEFAULT NULL,
  `row_id` int(11) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
}}}

*Note*:  Transfer tables are by default in the same schema as the table they update.  You can put them in a different schema using the stageSchemaPrefix property, which adds a prefix to the base schema name.  If the replicator cannot find a required transfer table, it will fail. 

To use transfer tables, select 'staged' as the loading method when configuring the BatchApplier in the replication service properties file.  You can change the prefix for staging tables, the row ID column name, and the table keys as part of BatchApplier configuration. 

= Character Sets =

Character sets are a headache in batch loading because all updates are written and read from CSV files, which can result in invalid transactions along the replication path.  Such problems are very difficult to debug.  Here are some tips to improve chances of happy replicating. 

  # Use UTF8 character sets consistently for all string and text data. 
  # Force Tungsten to convert data to Unicode rather than transfering strings.  I.e., specify `tungsten-installer ... --mysql-use-bytes-for-string=false`.  
  # When starting the replicator for MySQL replication, include the following command in the wrapper.conf file: `wrapper.java.additional.4=-Dfile.encoding=UTF8`. 

= Testing and Operation = 

You can test BatchApplier using the new croc utility from bristlecone.  Croc is documented in bristlecone/doc/CROC.TXT.  

You can also test using any load within the capabilities described in the next section.  Remember to run DDL on both master and slave databases. 

= Capabilities =

The BatchApplier implementation has the following general capabilities.  

  * The BatchApplier should correctly apply any sequence of insert and update equivalently to the normal fully serialized operation with SQL statements.  
  * INSERT, UPDATE, and DELETE statements should be handled.  
  * The following datatypes are supported:  INT, BIGINT, DATE, TIMESTAMP, VARCHAR, DOUBLE, FLOAT, DECIMAL. 
  * Replicator management functions, for example the trepctl, should function exactly as with normal fully serialized replication.  
  * Configuration parameters like the global buffer size (replicator.global.buffer.size) behave exactly as before.  In fact, this value should be increased to a very large value for the last stage so that the CSV files contain as many rows as possible at commit time.
  * The replicator will clean up CSV files and temp tables after commit and whenever it goes offline. 
  
= Limitations =

There are a number of limitations to the work done so far. 

  * The applier is still in active development.  Testing is limited to specific use cases. 
  * Deletes can cause problems.  For instance, issuing an INSERT followed by a DELETE on the same key value will be inverted if they execute in the same commit block.  BatchApplier will apply the DELETE first without effect, then INSERT the row.  
  * Not all datatypes are supported or even tested. 
  * Parallel replication has not been tested. 
  * When using staged loading with transfer tables, note that all tables must have a single key, that key must have the same name, and its value must be an INT.  

These limitations will be corrected in future builds. 