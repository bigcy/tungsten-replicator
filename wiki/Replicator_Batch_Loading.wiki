#summary Theory and Practice of Batch Loading using Tungsten

= Introduction =

Tungsten Replicator normally applies SQL changes to slaves by constructing SQL statements and executing in the exact order that transactions appear in the Tungsten History Log (THL).  This works well for OLTP databases like MySQL, PostgreSQL, Oracle, and MongoDB.  However, it is a poor approach for data warehouses. 

Data warehouse products like Vertica or GreenPlum load very slowly through JDBC interfaces (50 times slower or even more compared to MySQL).  Instead, we would really like to use a batch loader.  It turns out that both Vertica and Greenplum support the PostgreSQL COPY command, which allows data to be loaded quickly from CSV.  

Batch loading is good for OLTP databases, too.  MySQL's LOAD DATA INFILE can operate 20x faster than SQL inserts in some cases.  This makes it desirable for loading row updates quickly.  

The Tungsten [http://code.google.com/p/tungsten-replicator/source/browse/trunk/replicator/src/java/com/continuent/tungsten/replicator/applier/batch/BatchApplier.java BatchApplier] class is the first implementation of batch loading for Tungsten Replicator.  The rest of this file describes how it works. 

= Theory of Operation =

Java class BatchApplier loads data into the slave DBMS using CVS files and LOAD DATA INFILE.  The first implementation works on MySQL and PostgreSQL databases.  Here is the basic algorithm.  

While executing within a commit block, we write incoming transactions into open CSV files written by class CsvWriter.   There is a maximum of one insert CSV and one delete CSV file per database table. 
  * Inserts go into an insert CSV file for their table.   (The insert file contains the column values.) 
  * Deletes go into a delete CSV file for their table.  (The delete file contains only keys.) 
  * Updates put keys into the delete file and the updated values (i.e., row after images) into the insert file.  
  * Statements are ignored.   If you want DDL you need to put it in yourself. 

There are two ways to commit data.  The *direct* load method works by loading data as follows when we commit transactions.  
  # Close each delete CSV and load it into a temp table.  Delete any row in the corresponding real table that matches a key in the temp table. 
  # Close each insert CSV and load it into the real table. 
This method works for databases like MySQL that support REPLACE semantics in load files.  REPLACE means that if a row inserts into an existing row it replaces its contents.  

The *staged* load method loads via specially named staging tables.  In this technique, both inserts and deletes are loaded into permanent staging tables for each base table.  The delete stage table just contains keys, which as with the temp table used in the direct method can join with base rows to remove them.  The insert stage table contains rows to be inserted.  These are merged in using a specially constructed SQL command that eliminates duplicate rows.  

Staged loading is suitable for databases like PostgreSQL that do not support REPLACE semantics in load files.  This means that an INSERT followed by an UPDATE will result in two INSERTs on the same key in the base table, hence a primary key violation.  

So much for the algorithm.  If there are no delete operations this approach is equivalent to serial execution of the SQL operations but theoretically much faster because it avoids a lot of network round-trips and as well as overhead of the SQL itself.

= Batch Applier Setup =

Here is how to set up on MySQL.  This will be made easier in future 
when the batch loader is added to the installations. 

1. Enable row replication on the MySQL master using {{{set global binlog_format=row}}} or by updating my.cnf.  

2. Create a normal master/slave pair.  Here's a typical installation command.  The options for validation checks are necessary due a problem in the tungsten-installer.  

{{{
tools/tungsten-installer --master-slave -a \
  --master-host=logos1  \
  --datasource-user=tungsten  \
  --datasource-password=secret  \
  --service-name=percona \
  --home-directory=/opt/rhodges4 \
  --cluster-hosts=logos1,logos2 \
  --channels=1 \
  --buffer-size=100 \
  --mysql-use-bytes-for-string=false \
  --skip-validation-check=MySQLConfigFileCheck \
  --skip-validation-check=MySQLApplierServerIDCheck \
  --svc-parallelization-type=disk --start-and-report 
}}}

Once the slave is set up, go to that host and manually edit the replication service properties file (static-percona.properties in my example) as follows: 

1. Find the slave pipeline definition and set the following property.  The pkey and colnames filters are required so you get the proper names for columns and reduce the keys to only the columns that are actually primary keys.   Note:  you don't want to eliminate extra update columns as you are going to insert the full row. 

{{{
replicator.stage.q-to-dbms.filters=pkey,colnames,bidiSlave
}}}

2. Find the applier section and comment out or delete whatever is there.  Replace it with the following: 

{{{
# Batch applier basic configuration information. 
replicator.applier.dbms=com.continuent.tungsten.replicator.applier.batch.BatchApplier
replicator.applier.dbms.url=jdbc:mysql:thin://${replicator.global.db.host}:${replicator.global.db.port}/tungsten_${service.name}?createDB=true
replicator.applier.dbms.driver=org.drizzle.jdbc.DrizzleDriver
replicator.applier.dbms.user=${replicator.global.db.user}
replicator.applier.dbms.password=${replicator.global.db.password}

# Timezone and character set. 
replicator.applier.dbms.timezone=GMT+0:00
replicator.applier.dbms.charset=UTF-8

# Load method, COPY command template, and staging directory location. 
replicator.applier.dbms.loadMethod=direct
replicator.applier.dbms.loadBatchTemplate=LOAD DATA INFILE '%%FILE%%' REPLACE INTO TABLE %%TABLE%% CHARACTER SET utf8 FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '"'
replicator.applier.dbms.stageDirectory=/tmp/staging
replicator.applier.dbms.supportsReplace=true

# Extra parameters for loading via stage tables. 
replicator.applier.dbms.stageTablePrefix=stage_xxx
replicator.applier.dbms.stageInsertFromTemplate=INSERT INTO %%BASE_TABLE%%(%%BASE_COLUMNS%%) SELECT %%BASE_COLUMNS%% FROM %%STAGE_TABLE%%
replicator.applier.dbms.stageDeleteFromTemplate=DELETE FROM %%BASE_TABLE%% WHERE %%BASE_PKEY%% IN (SELECT %%STAGE_PKEY%% FROM %%STAGE_TABLE%%)
replicator.applier.dbms.stagePkeyColumn=id
replicator.applier.dbms.stageRowIdColumn=row_id
replicator.applier.dbms.cleanUpFiles=false
}}}

The properties that begin with "stage" are required only if you select staged as the load method.  There are no defaults for these values.  You must specify them fully or replication will fail.  

As shown in the example, SQL templates can be parameterized by values that Tungsten Replicator fills in at runtime.  The example shows all the currently supported values.  

Restart the slave replicator and batch loading is enabled.  From there on out you can start to test. 

= Transfer Table Format =

Transfer tables must follow a specific format that mimics the base table to which they apply.  There is a separate insert and delete table for each base table.  

For example, suppose we have a base table created by the following CREATE TABLE command: 

{{{
CREATE TABLE `croc_insertfloat` (
  `id` int(11) NOT NULL,
  `f_data` float DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
}}}

To make inserts on this table you must create an insert transfer table that has the same format including column names, types, and order.  Beyond that there are two differences.  First, the table should not have a primary key or you will get primary key violations if there are multiple inserts on the same row.  Second, the last column must contain a row_id column, which is used to distinguish between multiple inserts.  Here is the corresponding table matching the preceding example: 

{{{
CREATE TABLE `stage_xxx_insert_croc_insertfloat` (
  `id` int(11) NOT NULL,
  `f_data` float DEFAULT NULL,
  `row_id` int(11) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
}}}

To make deletes (or updates) on the table you must create a delete transfer table that contains only the primary key from the base table plus a row ID.  This looks like the following.  

{{{
CREATE TABLE `stage_xxx_delete_croc_insertfloat` (
  `id` int(11) DEFAULT NULL,
  `row_id` int(11) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
}}}

*Note*:  Transfer tables are by default in the same schema as the table they update.  You can put them in a different schema using the stageSchemaPrefix property, which adds a prefix to the base schema name.  If the replicator cannot find a required transfer table, it will fail. 

To use transfer tables, select 'staged' as the loading method when configuring the BatchApplier in the replication service properties file.  You can change the prefix for staging tables, the row ID column name, and the table keys as part of BatchApplier configuration. 

= Character Sets =

Character sets are a headache in batch loading because all updates are written and read from CSV files, which can result in invalid transactions along the replication path.  Such problems are very difficult to debug.  Here are some tips to improve chances of happy replicating. 

  # Use UTF8 character sets consistently for all string and text data. 
  # Force Tungsten to convert data to Unicode rather than transfering strings.  I.e., specify `tungsten-installer ... --mysql-use-bytes-for-string=false`.  
  # When starting the replicator for MySQL replication, include the following command in the wrapper.conf file: `wrapper.java.additional.4=-Dfile.encoding=UTF8`. 

= Testing and Operation = 

You can test BatchApplier using the new croc utility from bristlecone.  Croc is documented in bristlecone/doc/CROC.TXT.  

You can also test using any load within the capabilities described in the next section.  Remember to run DDL on both master and slave databases. 

= Capabilities =

The BatchApplier implementation has the following general capabilities.  

  * The BatchApplier should correctly apply any sequence of insert and update equivalently to the normal fully serialized operation with SQL statements.  
  * INSERT, UPDATE, and DELETE statements should be handled.  
  * The following datatypes are supported:  INT, BIGINT, DATE, TIMESTAMP, VARCHAR, DOUBLE, FLOAT, DECIMAL. 
  * Replicator management functions, for example the trepctl, should function exactly as with normal fully serialized replication.  
  * Configuration parameters like the global buffer size (replicator.global.buffer.size) behave exactly as before.  In fact, this value should be increased to a very large value for the last stage so that the CSV files contain as many rows as possible at commit time.
  * The replicator will clean up CSV files and temp tables after commit and whenever it goes offline. 
  
= Limitations =

There are a number of limitations to the work done so far. 

  * The applier is still in active development.  Testing is limited to specific use cases. 
  * Deletes can cause problems.  For instance, issuing an INSERT followed by a DELETE on the same key value will be inverted if they execute in the same commit block.  BatchApplier will apply the DELETE first without effect, then INSERT the row.  
  * Not all datatypes are supported or even tested. 
  * Parallel replication has not been tested. 
  * When using staged loading with transfer tables, note that all tables must have a single key, that key must have the same name, and its value must be an INT.  

These limitations will be corrected in future builds. 