#summary Tungsten Replicator cookbook
#labels Featured,Phase-Deploy
<wiki:toc max_depth="4" />

= Tungsten Replicator Cookbook =

This is a collection of practical recipes to deal with Tungsten Replicator.

== Basic Installation ==

=== Get the replicator ===

All the recipes about installation require that you get Tungsten Replicator binaries. This recipe explains how to do it once for ever.

 # Determine which is the latest version. The latest stable version is the one featured on [http://code.google.com/p/tungsten-replicator/ Tungsten Replicator project page]
 # If you want the latest release, which probably has more features, more bug fixes, and possibly more unknown bugs, you may get the very latest binaries [http://s3.amazonaws.com/files.continuent.com/builds/nightly/tungsten-2.0-snapshots/index.html from the build server].
 # Download the replicator
 # Create a temporary directory. This is *not* the directory where you want to install Tungsten. You will use it only to launch the installation command. 
 # Expand the tarball
 # Get inside the newly created directory
 # We refer to this directory as the *release directory*
 # Now you can follow the directions from another recipe.

=== Install a master / slave cluster ===

[https://lh5.googleusercontent.com/-jgCZbwwB4WE/TfC6Ksv2LiI/AAAAAAAABIQ/Ygpazw8EvEQ/s640/master_slave.png]

 # make sure that all the hosts meet the [https://s3.amazonaws.com/releases.continuent.com/doc/replicator-2.0.4/html/Tungsten-Installation-Guide-mysql/content/ch05.html requirements]
 # create a directory where you want to install the replicator. We refer to this directory as the * Tungsten home directory*;
 # Get the replicator binaries using [TungstenReplicatorCookbook#Get_the_replicator]
 # Run the following command, after changing the directory and host names to match your environment.

{{{
  TUNGSTEN_HOME=$HOME/replication
  MASTER=m1.mynetwork.com
  SLAVE1=m2.mynetwork.com
  SLAVE2=m3.mynetwork.com
  ./tools/tungsten-installer \
    --master-slave \
    --master-host=$MASTER \
    --datasource-user=MYUSER \
    --datasource-password=MYPWD \
    --service-name=dragon \
    --home-directory=$TUNGSTEN_HOME \
    --cluster-hosts=$MASTER,$SLAVE1,$SLAVE2 \
    --start-and-report
}}}

This command takes several defaults into account.
It assumes that MySQL is installed in {{{/var/lib/mysql/}}}, that the server uses port {{{3306}}}, and that the binary logs are in the data directory. We use these values because they are the ones used by RPM installers.
If you use different values, please see [TungstenReplicatorCookbook#Install_a_master_slave_directory_with_customized_parameters].


=== Install a master slave directory with customized parameters ===

If the defaults used in [TungstenReplicatorCookbook#Install_a_master_/_slave_cluster] are not convenient, you can instruct the replicator to use paths and ports that describe your server.

You should add these options to the installation command.

{{{
  --datasource-port=${MASTER_MYSQL_PORT} \
  --datasource-log-directory=/path/to/your/binary/logs \
  --datasource-log-pattern=my-bin
}}}


=== Install more than one Tungsten Replicator in one host ===

You may install more than one Tungsten Replicator in the same host.
One good reason for doing it may be because you want to test several servers at once, or because you have more MySQL servers in the same host and they need to replicate from different masters.
In the first case, you may want to have a look at [http://code.google.com/p/tungsten-toolbox/ Tungsten sandbox]. But if you want to do that on your own, you need to make sure that there is no overlapping of three base elements in the replicator: 

 * the *THL directory* (or the whole --home-directory), which is the place where the Transaction history logs are stored. This directory contains one directory for every service. It is important that you don't use two replicators with the same service name pointing at this directory.
 * The *THL port*. This is the port used by the replicator to send the transaction records.
 * The *RMI port*. This port is used to send JMX messages that the replicator uses for its own functioning.

For example, after installing Tungsten master/slave, I want to use an additional database with port 7102 and data directory somewhere else.

With the following command, I install a second replicator, using a different directory for the replicator, and pointing to the existing replicator

{{{
./tools/tungsten-installer \
  --master-slave \
  --cluster-hosts=127.0.0.1 \
  --master-host=HOST_NAME_OF_THE_MASTER \
  --master-thl-port=2112 \
  --datasource-port=7102 \
  --datasource-user=msandbox \
  --datasource-password=msandbox \
  --service-name=SAME_NAME_OF_THE_MASTER_SLAVE_SERVICE  \
  --home-directory=/tmp/some_place \
  --thl-port=4112 \
  --rmi-port=12000 \
  --start-and-report
}}}

Your replicator will end up in /tmp/some_place
To check the status you may run

{{{
/tmp/some_place/tungsten/tungsten-replicator/bin/trepctl -port 12000 services
}}}

=== Install a direct slave with parallel replication ===

[https://lh5.googleusercontent.com/-dhH-sPprwrw/TfC6L_F5RDI/AAAAAAAABIY/UwtVngp962Y/s720/slave_direct.png]

Direct slave replication is a quick method to set replication in a slave, without installing a master replicator.
It works by establishing a connection with the master, fetching the binary logs locally, and creating a THL stream with that.
The additional effort of creating relay logs is then compensated by using parallel replication.

This installation assumes that the slave is not running native MySQL replication. For that case, see [TungstenReplicatorCookbook#Taking_over_replication_from_a_MySQL_slave_in_direct_mode].

{{{
  TUNGSTEN_HOME=$HOME/replication
  MASTER=m1.mynetwork.com
  SLAVE1=m2.mynetwork.com
  ./tools/tungsten-installer \
    --direct \
    --master-host=$MASTER \
    --slave-host=$SLAVE \
    --master-user=tungsten \
    --slave-user=tungsten \
    --master-password=secret \
    --slave-password=secret \
    --service-name=mydirect \
    --channels=10 \
    --home-directory=$TUNGSTEN_BASE \
    --svc-parallelization-type=disk \
    --start-and-report 
}}}

Notice that you need to pass connection credentials for both the master and the slave. As in regular master/slave replication, there are defaults, which you can override using the following options (defaults in brackets)

{{{
  --master-host
  --master-port [3306]
  --master-user
  --master-password
  --master-log-directory  [/var/lib/mysql]
  --master-log-pattern    [mysql-bin]
}}}

For more information, see [http://scale-out-blog.blogspot.com/2011/08/adding-parallel-replication-to-mysql-in.html Adding parallel replication in a hurry].

=== Taking over replication from a MySQL slave in direct mode ===

In the previous recipe, we have shown how to start replication from a MySQL master, without a replicator on the master side. 
If the slave is already using MySQL native replication with the intended master, you can take over from the native replication stream simply adding to your installation command
{{{
--native-slave-takeover 
}}}

With this command, Tungsten stops the MySQL slave, gets the binlog file and position from the slave status, and starts replicating from that point.

When the replicator goes offline, it will send a "CHANGE MASTER TO" command to the slave to update log file and position, so that you can continue moving data either with Tungsten or with native replication.

=== Install Tungsten master/slave replication in a sandbox ===

[http://code.google.com/p/tungsten-toolbox/wiki/TungstenSandbox Tungsten Sandbox] is a dedicated script that allows you to install 
multiple dataabse servers and multiple Tungsten Replicator services in a single host.

[https://lh4.googleusercontent.com/-rHIfADez_jI/TfC6JQSSP8I/AAAAAAAABIM/7TtIX9XNAWw/s800/master_slave_sandbox.png]

Please refer to [http://code.google.com/p/tungsten-toolbox/wiki/TungstenSandbox Tungsten-Sandbox instructions] for more info.

=== Chain two replication clusters ===

Let's say that you have two replication clusters, one installed using [http://code.google.com/p/tungsten-replicator/w/edit.do#Install_a_master_/_slave_cluster the master slave recipe] and one using [http://code.google.com/p/tungsten-toolbox/ Tungsten sandbox].
You want the master of the Tungsten sandbox in your local host to become a slave of the master at m1.mynetwork.com.

[https://lh3.googleusercontent.com/-id11YtdENpY/TnZrvWcDHhI/AAAAAAAABMs/j4EFFqsKwTE/chaining_tungsten_clusters.png]

Go to the home directory of the tungsten sandbox master, and run this command:
{{{
cd $HOME/tdb2/db1
./tungsten/tools/configure-service -C \
  --local-service-name=tsandbox \
  --thl-port=12111 \
  --role=slave \
  --service-type=remote \
  --master-thl-host=m1.mynetwork.com \
  --master-thl-port=2112 \
  --datasource=127_0_0_1 \
  --svc-start \
  dragon
}}}

Notes:
 * The thl-port is the one defined by default for Tungsten Sandbox. If your cluster master uses a different THL, it myst be provided here;
 * local-service-name is the name of the service used by the Tungsten Sandbox master.

After this command, the two clusters are chained. Whatever is inserted into m1.mynetwork.com will also be inserted into the Tungsten sandbox master, and from there it will go to its slaves.

=== Modify one or more properties with the installer ===

Tungsten Replicator gets its configuration from a file called static-SERVICE_NAME.properties, located $TUNGSTEN_HOME/tungsten/tungsten-replicator/conf/.
This file can be edited with a regular text editor. If you know what you are doing, you can fine tune the replicator to your will.
The procedure is not painless. You need to install the replicator first, then edit the file, then eventually restart the replicator.

This procedure is difficult to script, and it is especially inconvenient when the changed property needs to be there right when the replicator starts.

To your help, there is an option of {{{tungsten-installer}}}, which allows you to change any property in the properties file.

For example, let's suppose that you don;t want the replicator to go ONLINE automatically when it starts. This behavior is controlled by a property called {{{replicator.auto_enable}}}, which is true by default. Of course, if you install the replicator with the --start option, you won't get a chance of modifying the property, because the replicator will be already online.

To achieve your purpose, you will add this option to the installation command:
{{{
    --property=replicator.auto_enable=false
}}}

=== Add one slave to an existing master ===

The procedure is almost the same used to create a master-slave cluster. You use a similar command, with {{{--master-slave}}} and {{{--master-host}}} as in the full cluster installation command. The difference is that the {{{--cluster-hosts}}} list will only contain the slave host. Tungsten will do the right thing.

=== Start a master service with a given binlog and position ===

The easiest way of starting the master service at a given binlog file and position is by using 

{{{ --master-log-file=mysql-bin.000045 --master-log-pos=10292 }}}

(Note: there is a problem with this procedure as documented in  [http://code.google.com/p/tungsten-replicator/issues/detail?id=216 Issue#216].

If the replicator has already been installed and we want to start replicating from a given binlog file and position, we can do this:

{{{
    trepctl -host host_name -service service_name offline 
    trepctl -host host_name -service service_name online -from-event 000045:10292
}}}
Notice that we don't give the complete binlog file name, but only the extension. The number after the colon is the position.

=== Modify the configuration template file prior to configuration ===

There are advanced scenarios where you would like to modify the source template files or even add additional templates to the code.  One of the simplest is when you need to add a new filter definition.  We recommend that you use diff/patch to modify the release package prior to configuration.

 * {{{cp tungsten-replicator-2.0.5 tungsten-replicator-2.0.5-diff}}}
 * Make changes in tungsten-replicator-2.0.5-diff
 * {{{diff -ruN tungsten-replicator-2.0.5 tungsten-replicator-2.0.5-diff > t-r-2.0.5.diff}}}
 * Apply the patch to the release package
  * {{{patch -p1 -dtungsten-replicator-2.0.5 < t-r-2.0.5.diff}}}
  * {{{cat t-r-2.0.5.diff | patch -p1 -dtungsten-replicator-2.0.5}}}
  * 
{{{
echo "diff -Nru tungsten-replicator-2.0.5/tungsten-replicator/samples/conf/filters/default/sample.tpl tungsten-replicator-2.0.5-diff/tungsten-replicator/samples/conf/filters/default/sample.tpl
--- tungsten-replicator-2.0.5/tungsten-replicator/samples/conf/filters/default/sample.tpl    1969-12-31 19:00:00.000000000 -0500
+++ tungsten-replicator-2.0.5-diff/tungsten-replicator/samples/conf/filters/default/sample.tpl 2011-09-16 12:13:01.000000000 -0400
@@ -0,0 +1,2 @@
+# Sample filter
+replicator.filter.sample=com.continuent.tungsten.replicator.filter.SampleFilter
\ No newline at end of file" | patch -p1 -dtungsten-replicator-2.0.5
}}}
 * {{{tungsten-replicator-2.0.5/tools/tungsten-installer ...}}}

== Multi-Master Installation ==

=== Install bi-directional replication ===

[https://lh5.googleusercontent.com/_gVfZHGgf5LA/TXO2MfOUagI/AAAAAAAABEc/jvt9lZC8uvY/Tungsten_bi_directional_replication.png]
To install multiple replication services on two nodes, you need first to install the master service. This is similar to master-slave installation, with the difference that we are not installing any slaves. Only masters.

{{{
TUNGSTEN_HOME=$HOME/replication
MASTER1=m1.mynetwork.com
MASTER1=m2.mynetwork.com

./tools/tungsten-installer \
    --master-slave \
    --master-host=$MASTER1 \
    --datasource-user=tungsten \
    --datasource-password=secret \
    --service-name=charlie \
    --home-directory=$TUNGSTEN_HOME \
    --cluster-hosts=$MASTER1 \
    --start-and-report

  ./tools/tungsten-installer \
    --master-slave \
    --master-host=$MASTER2 \
    --datasource-user=tungsten \
    --datasource-password=secret \
    --service-name=delta \
    --home-directory=$TUNGSTEN_HOME \
    --cluster-hosts=$MASTER2 \
    --start-and-report

}}}

You run the above commands from the *release directory*. After that, the *Tungsten home directory* is populated, and the following commands need to run from there.

What we need to do is creating, on each host, a slave service for the corresponding master on the other host. Thus, since we have "charlie" on MASTER1 and "delta" on MASTER2, we will create a slave service "delta" on MASTER1 and "charlie" on MASTER2.

{{{
TUNGSTEN_TOOLS=$TUNGSTEN_HOME/tungsten/tools

$TUNGSTEN_TOOLS/configure-service \
   --host $MASTER1 \
   -C -q \
   --local-service-name=charlie \
   --role=slave \
   --service-type=remote \
   --datasource=m1_mynetwork_com \
   --master-thl-host=$MASTER2 \
   --svc-start delta

$TUNGSTEN_TOOLS/configure-service 
  --host $MASTER2 \
  -C -q \
    --local-service-name=delta \
    --role=slave \
    --service-type=remote \
    --datasource=m2_mynetwork_com
    --master-thl-host=$MASTER1 \
    --svc-start charlie
}}}

The datasource name is the name of the local hostname, with dots replaced by underscores. (Note to self: this needs to become a bit easier to deal with).

=== Install bi-directional replication with an additional slave ===

The recipe is similar to [TungstenReplicatorCookbook#Install_bi-directional_replication].
Assuming that we have a candidate slave m3.mynetwork.com, which we want to be slave of m2, what we will do is simply add this information to the second installation command:

{{{
SLAVE1=m3.mynetwork.com

  ./tools/tungsten-installer \
    --master-slave \
    --master-host=$MASTER2 \
    --datasource-user=tungsten \
    --datasource-password=secret \
    --service-name=delta \
    --home-directory=$TUNGSTEN_HOME \
    --cluster-hosts=$MASTER2,SLAVE1 \
    --start-and-report
}}}

The only difference is SLAVE1 in the --cluster-hosts list, and Tungsten will take care of the rest.

=== Install a three masters replication ===

This recipe is left as an exercise for readers who have successfully managed [TungstenReplicatorCookbook#Install_a_four_masters_replication].

=== Install a four masters replication ===

[https://lh4.googleusercontent.com/-gAnEaWOQK40/Tm2NpEWCT0I/AAAAAAAABMc/M3zIn7PdIQg/s720/Tungsten_four_masters_replication.png]
You want to install four nodes, in such a way that each node is a master, and each one receives changes from all other nodes.

As in bi-directional replication, you install the master services first.

{{{
TUNGSTEN_HOME=$HOME/replication
MASTER1=m1.mynetwork.com
MASTER2=m2.mynetwork.com
MASTER3=m3.mynetwork.com
MASTER4=m4.mynetwork.com

./tools/tungsten-installer \
   --master-slave \
   --master-host=$MASTER1 \
   --datasource-user=tungsten \
   --datasource-password=secret \
   --service-name=alpha \
   --home-directory=/home/tungsten/installs/four_masters \
   --cluster-hosts=$MASTER1 --start-and-report

./tools/tungsten-installer \
   --master-slave \
   --master-host=$MASTER2 \
   --datasource-user=tungsten \
   --datasource-password=secret \
   --service-name=alpha \
   --home-directory=/home/tungsten/installs/four_masters \
   --cluster-hosts=$MASTER2 --start-and-report

./tools/tungsten-installer \
   --master-slave \
   --master-host=$MASTER3 \
   --datasource-user=tungsten \
   --datasource-password=secret \
   --service-name=alpha \
   --home-directory=/home/tungsten/installs/four_masters \
   --cluster-hosts=$MASTER3 --start-and-report

./tools/tungsten-installer \
   --master-slave \
   --master-host=$MASTER4 \
   --datasource-user=tungsten \
   --datasource-password=secret \
   --service-name=alpha \
   --home-directory=/home/tungsten/installs/four_masters \
   --cluster-hosts=$MASTER4 --start-and-report
}}}

Next, you will install three slave services for each master.

{{{
TUNGSTEN_TOOLS=$TUNGSTEN_HOME/tungsten/tools

# MASTER 1

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER1 \
  --datasource=m1_mynetwork_com \
  --local-service-name=alpha \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER2 \
  --svc-start bravo

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER1 \
  --datasource=m1_mynetwork_com \
  --local-service-name=alpha \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER3 \
  --svc-start charlie

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER1 \
  --datasource=m1_mynetwork_com \
  --local-service-name=alpha \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER4 \
  --svc-start delta
}}}

Let's see master 2 

{{{
# MASTER 2

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER2 \
  --datasource=m2_mynetwork_com \
  --local-service-name=bravo \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER1 \
  --svc-start alpha

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER2 \
  --datasource=m2_mynetwork_com \
  --local-service-name=bravo \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER3 \
  --svc-start charlie

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER2 \
  --datasource=m2_mynetwork_com \
  --local-service-name=bravo \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER4 \
  --svc-start delta
}}}

And master 3:

{{{
# MASTER 3

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER3 \
  --datasource=m3_mynetwork_com \
  --local-service-name=charlie \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER1 \
  --svc-start alpha

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER3 \
  --datasource=m3_mynetwork_com \
  --local-service-name=charlie \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER2 \
  --svc-start bravo

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER4 \
  --datasource=m3_mynetwork_com \
  --local-service-name=bravo \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER4 \
  --svc-start delta
}}}

And finally master 4:

{{{
# MASTER 4

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER4 \
  --datasource=m4_mynetwork_com \
  --local-service-name=delta \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER1 \
  --svc-start alpha

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER4 \
  --datasource=m4_mynetwork_com \
  --local-service-name=bravo \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER2 \
  --svc-start bravo

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER4 \
  --datasource=m4_mynetwork_com \
  --local-service-name=delta \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER3 \
  --svc-start charlie
}}}

The procedure is longish, but once you get the gist of it you will be able to make a loop instead of coding these commands manually.

== Heterogeneous Replication ==

=== PostgreSQL to PostgreSQL and MySQL Replication ===

The following examples show how to install a PostgreSQL -> PostgreSQL & MySQL topology on a single host.

==== Install a prototype PostgreSQL logical replicator (master) ====

Prepare the master PostgreSQL instance:

  # Download [http://slony.info/downloads/ Slony], extract it and build it. After doing that, there will be a slonik executable available, which will you need to specify via "--postgresql-slonik" property (as shown below).
  # Specify tables to replicate in "--postgresql-tables".

Here's a full example:

{{{
./tools/tungsten-installer --master-slave -a --cluster-hosts=127.0.0.1 \
--user=postgres \
--master-host=127.0.0.1 \
--datasource-type=postgresql \
--datasource-port=54321 \
--postgresql-dbname=pgbench \
--home-directory=/opt/pg2pg/tungsten/S \
--datasource-user=postgres \
--datasource-password=secret \
--service-name=fromslony \
--rmi-port=12020 \
--thl-port=12021 \
--postgresql-slonik=/opt/pg2pg/slony1-2.0.6/src/slonik/slonik \
--postgresql-tables=public.tt1,public.t \
--start-and-report
}}}

Note: the latter command will drop any existing Slony triggers and objects and then recreate them.

==== Install a PostgreSQL applier replicator (slave) ====

A slave is installed on a different PostgreSQL instance:

{{{
./tools/tungsten-installer --master-slave -a --cluster-hosts=127.0.0.1 \
--user=postgres \
--master-host=127.0.0.1 \
--datasource-type=postgresql \
--datasource-port=54323 \
--postgresql-dbname=pgbench \
--home-directory=/opt/pg2pg/tungsten/P \
--datasource-user=postgres \
--datasource-password=secret \
--service-name=fromslony \
--rmi-port=12022 \
--master-thl-port=12021 \
--thl-port=12023 \
--start-and-report
}}}

If replicating from a PostgreSQLSlonyExtractor enabled master, slave needs to have tables already created, as DDL is not replicated with this method.

==== Install a MySQL applier replicator (slave) ====

Note the filters we enable for MySQL slave to be able to apply PostgreSQL origin events:

{{{
./tools/tungsten-installer --master-slave -a --cluster-hosts=127.0.0.1 \
--user=tungsten \
--master-host=127.0.0.1 \
--datasource-type=mysql \
--datasource-port=12002 \
--mysql-enable-ansiquotes=true \
--mysql-enable-noonlykeywords=true \
--home-directory=/opt/pg2mysql/M \
--datasource-user=tungsten \
--datasource-password=secret \
--service-name=fromslony \
--rmi-port=12024 \
--master-thl-port=12021 \
--thl-port=12025 \
--start-and-report \
--no-validation
}}}

=== MySQL to PostgreSQL Replication ===

It is recommended to use ROW replication on the MySQL master, because pure SQL statements can have different dialect and fail on other type DBMS.

==== Install a MySQL Master ====

If you have ENUM types in the database, make sure you enable "--mysql-enable-enumtostring" option:

{{{
.tools/tungsten-installer --master-slave -a --cluster-hosts=127.0.0.1 \
--master-host=127.0.0.1 \
--user=tungsten \
--home-directory=/opt/mysql2others/M \
--datasource-port=12001 \
--datasource-user=tungsten \
--datasource-password=secret \
--service-name=frommysql \
--rmi-port=20000 \
--thl-port=12112 \
--mysql-enable-enumtostring=true \
--mysql-use-bytes-for-string=false \
--skip-validation-check=MySQLNoMySQLReplicationCheck \
--start-and-report \
--no-validation
}}}

Note: "--mysql-use-bytes-for-string=false" option ensures that character fields are transferred in a way that other DBMS types understand.

==== Install a PostgreSQL Slave ====

If you're planning to replicate some known DDL statements, you can enable pgddl.js filter with option "--postgresql-enable-mysql2pgddl" and extend it to include the statements you need to support:

{{{
.tools/tungsten-installer --master-slave -a --cluster-hosts=127.0.0.1 \
--user=postgres \
--master-host=127.0.0.1 \
--datasource-type=postgresql \
--datasource-port=54324 \
--postgresql-dbname=postgres \
--home-directory=/opt/mysql2pg/P \
--datasource-user=postgres \
--datasource-password=secret \
--service-name=frommysql \
--rmi-port=10012 \
--master-thl-port=12112 \
--thl-port=12111 \
--postgresql-enable-mysql2pgddl=true \
--start-and-report
}}}

=== MySQL to MongoDB Replication ===

The following examples show how to install MySQL to MongoDB replication. 

==== Install a MySQL Master ====

Install a MySQL master replicator.  MySQL must use ROW replication, so you must set binlog_format=row before enabling replication.  You must enable the colnames and pkey filter so that the MongoDB can generate column names and also generate MongoDB indexes on primary key values. 

{{{
tools/tungsten-installer --master-slave -a \
  --datasource-type=mysql \
  --master-host=logos1  \
  --datasource-user=tungsten  \
  --datasource-password=secret  \
  --service-name=mongodb \
  --home-directory=/opt/continuent \
  --cluster-hosts=logos1 \
  --mysql-use-bytes-for-string=false \
  --svc-extractor-filters=colnames,pkey \
  --svc-parallelization-type=disk --start-and-report
}}}

==== Install a MongoDB Slave ====

Install MongoDB 1.8.3 and then install a MongoDB slave.  Other MongoDB versions may work as well. 

{{{
tools/tungsten-installer --master-slave -a \
  --datasource-type=mongodb \
  --master-host=logos1  \
  --datasource-user=tungsten  \
  --datasource-password=secret  \
  --service-name=mongodb \
  --home-directory=/opt/continuent \
  --cluster-hosts=logos2 \
  --skip-validation-check=InstallerMasterSlaveCheck \
  --svc-parallelization-type=disk --start-and-report
}}}

MySQL tables will be materialized as collections in MongoDB.  Columns convert to BSON properties.  SQL primary keys will be converted to (composite) indexes on collections. 

== Administration ==

=== Check replication services ===

The tool that helps you look inside the replicator is called {{{trepctl}}}, and it's located under $TUNGSTEN_HOME/tungsten/tungsten-replicator/bin.

To get a list of the existing services, run:

{{{
TUNGSTEN_BIN=$TUNGSTEN_HOME/tungsten/tungsten-replicator/bin

$TUNGSTEN_BIN/trepctl services
}}}


=== Check replication status ===

The 'status' option of trepctl will give you detailed information about the health of the replicator.
{{{
$TUNGSTEN_BIN/trepctl -service service_name status
}}}


=== Suspend and resume replication ===

To suspend a replicator service, put it offline:

{{{
$TUNGSTEN_BIN/trepctl -service service_name offline
}}}

Notice that this operation will not affect other services, if your replicator is running more than one.

To put it back online, you change the keyword.

{{{
$TUNGSTEN_BIN/trepctl -service service_name online
}}}

You may go online with several options, such as skipping one or more transactions, or replicating from a different file/position.

==== Resuming replication and skipping a transaction ====

{{{
$TUNGSTEN_BIN/trepctl -service service_name online -skip-seqno 2340
}}}

=== Inspect Transaction History Logs ===

The THL (Transaction History Logs) is the data that Tungsten has taken from the master's binary logs and transported to its servers, with the addition of some metadata.

There is a tool, called 'thl', which allows you to see the status of the logs and eventually list its contents. 

{{{
$TUNGSTEN_BIN/thl -service service_name info
}}}
This command will give you a summary of what you may find in the THL.

{{{
$TUNGSTEN_BIN/thl -service service_name index
}}}
Similar to the previous one, it will give you a list of the available THL files, with the minimum and maximum transaction number for each file.

{{{
$TUNGSTEN_BIN/thl -service service_name list
}}}
This command will list all the transactions that are available for the given service. This is potentially dangerous, since it may list millions of transactions on your screen. See the next ones for more options.

{{{
$TUNGSTEN_BIN/thl -service service_name list -low 0 -high 1858
$TUNGSTEN_BIN/thl -service service_name list -seqno 34902
}}}
The first command lists transactions in a given range, while the second lists only one specific transaction.

=== Check parallel replication status ===

==== Shards ====

{{{
$TUNGSTEN_BIN/trepctl -service service_name status -name shards
}}}

==== Tasks ====

{{{
$TUNGSTEN_BIN/trepctl -service service_name status -name tasks
}}}

==== Stores ====

{{{
$TUNGSTEN_BIN/trepctl -service service_name status -name stores
}}}

=== Resetting the Transaction History Logs ===

The THL can be cleared in order to reset the replication service and start from a clean slate.

 * Suspend replication
{{{
$TUNGSTEN_BIN/trepctl -service service_name offline
}}}
 * Remove all existing THL files
{{{
rm $TUNGSTEN_HOME/thl/service_name/*
}}}
 * Remove all existing relay files
{{{
rm $TUNGSTEN_HOME/relay/service_name/*
}}}
 * Reset the replication service schema
{{{
TRUNCATE TABLE tungsten_service_name.trep_commit_seqno;
TRUNCATE TABLE tungsten_service_name.heartbeat;
}}}
 * Resume replication
{{{
$TUNGSTEN_BIN/trepctl -service service_name online
}}}

=== Upgrading from Tungsten Replicator 2.0.4 to 2.0.5 ===

Tungsten Replicator is released quite frequently. We add planned features and fix bugs continuously. Therefore, getting [http://s3.amazonaws.com/files.continuent.com/builds/nightly/tungsten-2.0-snapshots/index.html the latest release] is very important.

If you have installed Tungsten Replicator 2.0.4 or later, and you want to upgrade to the latest version, here's what to do.

After you [http://code.google.com/p/tungsten-replicator/w/edit.do#Get_the_replicator get the replicator], run these commands for each host:

{{{
ssh host_name $TUNGSTEN_HOME/tungsten/tungsten-replicator/bin/replicator stop
./tools/update --host=host_name \
   --user=$USER \
   --release-directory=$TUNGSTEN_HOME
 
$TUNGSTEN_HOME/tungsten/tungsten-replicator/bin/trepctl -host host_name services
}}}

For example, if you have installed a regular master slave cluster using [http://code.google.com/p/tungsten-replicator/w/edit.do#Install_a_master_/_slave_cluster this recipe], the release directory will be $HOME/replication.

If the replicator is offline after the update command (that depends on your settings), you can simply put it online using trepctl.


*Notes:* 
  * Upgrading from previous version of Tungsten Replicator requires a manual re-installation, because of the new installer format.
Upgrading to the next version of Tungsten should be handled in the same way described in this section.
  * The host used in the above commands must be the same used when the server was defined. If a host can be addressed by two names (e.g _m1.mynetwork.com_ and _m1_) the same name that was used during installation must also be used for the upgrade.

=== Increasing the JVM memory size ===

The {{{--java-mem-size}}} option sets the amount of memory allocated to the JVM.  You can update the allocation on an existing replicator using the {{{tools/update}}} script.

{{{
$TUNGSTEN_HOME/tungsten/tools/update -a --java-mem-size=1024
$TUNGSTEN_HOME/tungsten/tungsten-replicator/bin/replicator restart
$TUNGSTEN_HOME/tungsten/tungsten-replicator/bin/trepctl services
}}}

=== Managing replicator log space ===

There are 3 kinds of logs used in Tungsten Replicator.
 * Master Relay Logs
 * Transaction History Logs
 * Service Logs

==== Master Relay Logs ===

The Master Relay Logs (MRL) are used when the master is extracting events from a MySQL server.  The MRL files are stored in the {{{$TUNGSTEN_HOME/relay/service_name}}} directory.  Use a symlink if you would like to store these files on a different disk.

The {{{replicator.extractor.dbms.relayLogRetention}}} value in {{{tungsten-replicator/conf/static-service_name.properties}}} defines how many MRL files are kept after the replicator service has extracted events from it.  You can configure this setting using the {{{--property}}} flag.

{{{
# New Installation
./tools/tungsten-installer ... --property=replicator.extractor.dbms.relayLogRetention=4

# Existing Installation
$TUNGSTEN_HOME/tools/configure-service -U --property=replicator.extractor.dbms.relayLogRetention=4 service_name
$TUNGSTEN_HOME/tungsten-replicator/bin/trepctl -service service_name stop
$TUNGSTEN_HOME/tungsten-replicator/bin/trepctl -service service_name start
$TUNGSTEN_HOME/tungsten-replicator/bin/trepctl -service service_name status
}}}

==== Transaction History Logs ====

The Transaction History Logs (THL) are used to store events after they are extracted, to transmit events between the master and slave and to store events prior to being applied.  The THL files are stored in the {{{$TUNGSTEN_HOME/thl/service_name}}} directory.  Use a symlink if you would like to store these files on a different disk.

The {{{replicator.store.thl.log_file_retention}}} value in {{{tungsten-replicator/conf/static-service_name.properties}}} defines how long THL files are kept.  You can configure this setting using the {{{--property}}} flag.

{{{
# New Installation
./tools/tungsten-installer ... --property=replicator.store.thl.log_file_retention=3d

# Existing Installation
$TUNGSTEN_HOME/tools/configure-service -U --property=replicator.store.thl.log_file_retention=3d service_name
$TUNGSTEN_HOME/tungsten-replicator/bin/trepctl -service service_name stop
$TUNGSTEN_HOME/tungsten-replicator/bin/trepctl -service service_name start
$TUNGSTEN_HOME/tungsten-replicator/bin/trepctl -service service_name status
}}}

==== Service Logs ====

The log files are stored in {{{tungsten-replicator/log}}}.  There are two service logs maintained by the replicator.

 * {{{user.log}}} - Partial service log that includes state changes and basic error messages
 * {{{trepsvc.log}}} - Complete service log including stack traces

The settings for these log files are located in {{{tungsten-replicator/conf/log4j.properties}}}.  You can modify the {{{log4j.appender.file.MaxFileSize}}} and {{{log4j.appender.file.MaxBackupIndex}}} values for each log file to manage how much disk space is required to hold the log files.

Check out the [TungstenReplicatorCookbook#Modify_the_configuration_template_file_prior_to_configuration] recipe for an idea of how you can update the {{{log4j.properties}}} file as part of installation.

Short link to this page: [http://bit.ly/tr20_cookbook http://bit.ly/tr20_cookbook]

== Monitoring and Troubleshooting ==

=== Monitor JVM memory usage ===

It is possible to use JConsole to monitor the memory usage of the Tungsten Replicator process.  If you see that the allocated memory is filling up, you may increase it using the {{--java-mem-size}} argument.

JConsole requires that jmxremote is enabled in {{{tungsten-replicator/conf/wrapper.conf}}}.  It is enabled by default but make sure that {{{wrapper.java.additional.3=-Dcom.sun.management.jmxremote}}} is not commented out in the file.

You can get information on JConsole at [http://java.sun.com/developer/technicalArticles/J2SE/jconsole.html].