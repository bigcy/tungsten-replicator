#summary Tungsten Replicator cookbook
#labels Featured,Phase-Deploy

= Tungsten Replicator Cookbook =
 
This is a collection of practical recipes to deal with Tungsten Replicator.

  * [TRCBasicInstallation Basic installation]
    * [http://code.google.com/p/tungsten-replicator/wiki/TRCBasicInstallation?ts=1317307167&updated=TRCBasicInstallation#Install_a_master_/_slave_cluster Install a master / slave cluster]
    * [http://code.google.com/p/tungsten-replicator/wiki/TRCBasicInstallation?ts=1317307317&updated=TRCBasicInstallation#Get_the_replicator Get the replicator]
    * [http://code.google.com/p/tungsten-replicator/wiki/TRCBasicInstallation?ts=1317307317&updated=TRCBasicInstallation#Install_a_master_slave_directory_with_customized_parameters Install a master slave directory with customized parameters]
    * [http://code.google.com/p/tungsten-replicator/wiki/TRCBasicInstallation?ts=1317307317&updated=TRCBasicInstallation#Install_more_than_one_Tungsten_Replicator_in_one_host Install more than one Tungsten Replicator in one host]
    * [http://code.google.com/p/tungsten-replicator/wiki/TRCBasicInstallation?ts=1317307317&updated=TRCBasicInstallation#Install_a_direct_slave_with_parallel_replication Install a direct slave with parallel replication]
    * [http://code.google.com/p/tungsten-replicator/wiki/TRCBasicInstallation?ts=1317307317&updated=TRCBasicInstallation#Taking_over_replication_from_a_MySQL_slave_in_direct_mode Taking over replication from a MySQL slave in direct mode]
    * [http://code.google.com/p/tungsten-replicator/wiki/TRCBasicInstallation?ts=1317307317&updated=TRCBasicInstallation#Install_Tungsten_master/slave_replication_in_a_sandbox Install Tungsten master/slave replication in a sandbox]
    * [http://code.google.com/p/tungsten-replicator/wiki/TRCBasicInstallation?ts=1317307317&updated=TRCBasicInstallation#Chain_two_replication_clusters Chain two replication clusters]
    * [http://code.google.com/p/tungsten-replicator/wiki/TRCBasicInstallation?ts=1317307317&updated=TRCBasicInstallation#Modify_one_or_more_properties_with_the_installer Modify one or more properties with the installer]
    * [http://code.google.com/p/tungsten-replicator/wiki/TRCBasicInstallation?ts=1317307317&updated=TRCBasicInstallation#Add_one_slave_to_an_existing_master Add one slave to an existing master]
    * [http://code.google.com/p/tungsten-replicator/wiki/TRCBasicInstallation?ts=1317307317&updated=TRCBasicInstallation#Start_a_master_service_with_a_given_binlog_and_position Start a master service with a given binlog and position]
    * [http://code.google.com/p/tungsten-replicator/wiki/TRCBasicInstallation?ts=1317307317&updated=TRCBasicInstallation#Modify_the_configuration_template_file_prior_to_configuration Modify the configuration template file prior to configuration]

== Multi-Master Installation ==

=== Install bi-directional replication ===

[https://lh5.googleusercontent.com/_gVfZHGgf5LA/TXO2MfOUagI/AAAAAAAABEc/jvt9lZC8uvY/Tungsten_bi_directional_replication.png]
To install multiple replication services on two nodes, you need first to install the master service. This is similar to master-slave installation, with the difference that we are not installing any slaves. Only masters.

{{{
TUNGSTEN_HOME=$HOME/replication
MASTER1=m1.mynetwork.com
MASTER1=m2.mynetwork.com

./tools/tungsten-installer \
    --master-slave \
    --master-host=$MASTER1 \
    --datasource-user=tungsten \
    --datasource-password=secret \
    --service-name=charlie \
    --home-directory=$TUNGSTEN_HOME \
    --cluster-hosts=$MASTER1 \
    --start-and-report

  ./tools/tungsten-installer \
    --master-slave \
    --master-host=$MASTER2 \
    --datasource-user=tungsten \
    --datasource-password=secret \
    --service-name=delta \
    --home-directory=$TUNGSTEN_HOME \
    --cluster-hosts=$MASTER2 \
    --start-and-report

}}}

You run the above commands from the *release directory*. After that, the *Tungsten home directory* is populated, and the following commands need to run from there.

What we need to do is creating, on each host, a slave service for the corresponding master on the other host. Thus, since we have "charlie" on MASTER1 and "delta" on MASTER2, we will create a slave service "delta" on MASTER1 and "charlie" on MASTER2.

{{{
TUNGSTEN_TOOLS=$TUNGSTEN_HOME/tungsten/tools

$TUNGSTEN_TOOLS/configure-service \
   --host $MASTER1 \
   -C -q \
   --local-service-name=charlie \
   --role=slave \
   --service-type=remote \
   --datasource=m1_mynetwork_com \
   --master-thl-host=$MASTER2 \
   --svc-start delta

$TUNGSTEN_TOOLS/configure-service 
  --host $MASTER2 \
  -C -q \
    --local-service-name=delta \
    --role=slave \
    --service-type=remote \
    --datasource=m2_mynetwork_com
    --master-thl-host=$MASTER1 \
    --svc-start charlie
}}}

The datasource name is the name of the local hostname, with dots replaced by underscores. (Note to self: this needs to become a bit easier to deal with).

=== Install bi-directional replication with an additional slave ===

The recipe is similar to [TungstenReplicatorCookbook#Install_bi-directional_replication].
Assuming that we have a candidate slave m3.mynetwork.com, which we want to be slave of m2, what we will do is simply add this information to the second installation command:

{{{
SLAVE1=m3.mynetwork.com

  ./tools/tungsten-installer \
    --master-slave \
    --master-host=$MASTER2 \
    --datasource-user=tungsten \
    --datasource-password=secret \
    --service-name=delta \
    --home-directory=$TUNGSTEN_HOME \
    --cluster-hosts=$MASTER2,SLAVE1 \
    --start-and-report
}}}

The only difference is SLAVE1 in the --cluster-hosts list, and Tungsten will take care of the rest.

=== Install a three masters replication ===

This recipe is left as an exercise for readers who have successfully managed [TungstenReplicatorCookbook#Install_a_four_masters_replication].

=== Install a four masters replication ===

[https://lh4.googleusercontent.com/-gAnEaWOQK40/Tm2NpEWCT0I/AAAAAAAABMc/M3zIn7PdIQg/s720/Tungsten_four_masters_replication.png]
You want to install four nodes, in such a way that each node is a master, and each one receives changes from all other nodes.

As in bi-directional replication, you install the master services first.

{{{
TUNGSTEN_HOME=$HOME/replication
MASTER1=m1.mynetwork.com
MASTER2=m2.mynetwork.com
MASTER3=m3.mynetwork.com
MASTER4=m4.mynetwork.com

./tools/tungsten-installer \
   --master-slave \
   --master-host=$MASTER1 \
   --datasource-user=tungsten \
   --datasource-password=secret \
   --service-name=alpha \
   --home-directory=/home/tungsten/installs/four_masters \
   --cluster-hosts=$MASTER1 --start-and-report

./tools/tungsten-installer \
   --master-slave \
   --master-host=$MASTER2 \
   --datasource-user=tungsten \
   --datasource-password=secret \
   --service-name=alpha \
   --home-directory=/home/tungsten/installs/four_masters \
   --cluster-hosts=$MASTER2 --start-and-report

./tools/tungsten-installer \
   --master-slave \
   --master-host=$MASTER3 \
   --datasource-user=tungsten \
   --datasource-password=secret \
   --service-name=alpha \
   --home-directory=/home/tungsten/installs/four_masters \
   --cluster-hosts=$MASTER3 --start-and-report

./tools/tungsten-installer \
   --master-slave \
   --master-host=$MASTER4 \
   --datasource-user=tungsten \
   --datasource-password=secret \
   --service-name=alpha \
   --home-directory=/home/tungsten/installs/four_masters \
   --cluster-hosts=$MASTER4 --start-and-report
}}}

Next, you will install three slave services for each master.

{{{
TUNGSTEN_TOOLS=$TUNGSTEN_HOME/tungsten/tools

# MASTER 1

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER1 \
  --datasource=m1_mynetwork_com \
  --local-service-name=alpha \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER2 \
  --svc-start bravo

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER1 \
  --datasource=m1_mynetwork_com \
  --local-service-name=alpha \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER3 \
  --svc-start charlie

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER1 \
  --datasource=m1_mynetwork_com \
  --local-service-name=alpha \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER4 \
  --svc-start delta
}}}

Let's see master 2 

{{{
# MASTER 2

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER2 \
  --datasource=m2_mynetwork_com \
  --local-service-name=bravo \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER1 \
  --svc-start alpha

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER2 \
  --datasource=m2_mynetwork_com \
  --local-service-name=bravo \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER3 \
  --svc-start charlie

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER2 \
  --datasource=m2_mynetwork_com \
  --local-service-name=bravo \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER4 \
  --svc-start delta
}}}

And master 3:

{{{
# MASTER 3

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER3 \
  --datasource=m3_mynetwork_com \
  --local-service-name=charlie \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER1 \
  --svc-start alpha

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER3 \
  --datasource=m3_mynetwork_com \
  --local-service-name=charlie \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER2 \
  --svc-start bravo

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER4 \
  --datasource=m3_mynetwork_com \
  --local-service-name=bravo \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER4 \
  --svc-start delta
}}}

And finally master 4:

{{{
# MASTER 4

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER4 \
  --datasource=m4_mynetwork_com \
  --local-service-name=delta \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER1 \
  --svc-start alpha

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER4 \
  --datasource=m4_mynetwork_com \
  --local-service-name=bravo \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER2 \
  --svc-start bravo

$TUNGSTEN_TOOLS/configure-service \
  -C --quiet \
  --host=$MASTER4 \
  --datasource=m4_mynetwork_com \
  --local-service-name=delta \
  --role=slave \
  --service-type=remote \
  --release-directory=$TUNGSTEN_HOME/tungsten \
  --master-thl-host=$MASTER3 \
  --svc-start charlie
}}}

The procedure is longish, but once you get the gist of it you will be able to make a loop instead of coding these commands manually.

== Heterogeneous Replication ==

=== PostgreSQL to PostgreSQL and MySQL Replication ===

The following examples show how to install a PostgreSQL -> PostgreSQL & MySQL topology on a single host.

==== Install a prototype PostgreSQL logical replicator (master) ====

Prepare the master PostgreSQL instance:

  # Download [http://slony.info/downloads/ Slony], extract it and build it. After doing that, there will be a slonik executable available, which will you need to specify via "--postgresql-slonik" property (as shown below).
  # Specify tables to replicate in "--postgresql-tables".

Here's a full example:

{{{
./tools/tungsten-installer --master-slave -a --cluster-hosts=127.0.0.1 \
--user=postgres \
--master-host=127.0.0.1 \
--datasource-type=postgresql \
--datasource-port=54321 \
--postgresql-dbname=pgbench \
--home-directory=/opt/pg2pg/tungsten/S \
--datasource-user=postgres \
--datasource-password=secret \
--service-name=fromslony \
--rmi-port=12020 \
--thl-port=12021 \
--postgresql-slonik=/opt/pg2pg/slony1-2.0.6/src/slonik/slonik \
--postgresql-tables=public.tt1,public.t \
--start-and-report
}}}

Note: the latter command will drop any existing Slony triggers and objects and then recreate them.

==== Install a PostgreSQL applier replicator (slave) ====

A slave is installed on a different PostgreSQL instance:

{{{
./tools/tungsten-installer --master-slave -a --cluster-hosts=127.0.0.1 \
--user=postgres \
--master-host=127.0.0.1 \
--datasource-type=postgresql \
--datasource-port=54323 \
--postgresql-dbname=pgbench \
--home-directory=/opt/pg2pg/tungsten/P \
--datasource-user=postgres \
--datasource-password=secret \
--service-name=fromslony \
--rmi-port=12022 \
--master-thl-port=12021 \
--thl-port=12023 \
--start-and-report
}}}

If replicating from a PostgreSQLSlonyExtractor enabled master, slave needs to have tables already created, as DDL is not replicated with this method.

==== Install a MySQL applier replicator (slave) ====

Note the filters we enable for MySQL slave to be able to apply PostgreSQL origin events:

{{{
./tools/tungsten-installer --master-slave -a --cluster-hosts=127.0.0.1 \
--user=tungsten \
--master-host=127.0.0.1 \
--datasource-type=mysql \
--datasource-port=12002 \
--mysql-enable-ansiquotes=true \
--mysql-enable-noonlykeywords=true \
--home-directory=/opt/pg2mysql/M \
--datasource-user=tungsten \
--datasource-password=secret \
--service-name=fromslony \
--rmi-port=12024 \
--master-thl-port=12021 \
--thl-port=12025 \
--start-and-report
}}}

=== MySQL to PostgreSQL and Oracle Replication ===

It is recommended to use ROW replication on the MySQL master, because pure SQL statements can have different dialect and fail on other type DBMS.

==== Install a MySQL Master ====

If you have ENUM types in the database, make sure you enable "--mysql-enable-enumtostring" option:

{{{
./tools/tungsten-installer --master-slave -a --cluster-hosts=127.0.0.1 \
--master-host=127.0.0.1 \
--user=tungsten \
--home-directory=/opt/mysql2others/M \
--datasource-port=12001 \
--datasource-user=tungsten \
--datasource-password=secret \
--service-name=frommysql \
--rmi-port=20000 \
--thl-port=12112 \
--mysql-enable-enumtostring=true \
--mysql-use-bytes-for-string=false \
--skip-validation-check=MySQLNoMySQLReplicationCheck \
--start-and-report
}}}

Note: "--mysql-use-bytes-for-string=false" option ensures that character fields are transferred in a way that other DBMS types understand.

==== Install a PostgreSQL Slave ====

If you're planning to replicate some known DDL statements, you can enable pgddl.js filter with option "--postgresql-enable-mysql2pgddl" and extend it to include the statements you need to support:

{{{
./tools/tungsten-installer --master-slave -a --cluster-hosts=127.0.0.1 \
--user=postgres \
--master-host=127.0.0.1 \
--datasource-type=postgresql \
--datasource-port=54324 \
--postgresql-dbname=postgres \
--home-directory=/opt/mysql2pg/P \
--datasource-user=postgres \
--datasource-password=secret \
--service-name=frommysql \
--rmi-port=10012 \
--master-thl-port=12112 \
--thl-port=12111 \
--postgresql-enable-mysql2pgddl=true \
--start-and-report
}}}

==== Install an Oracle Slave ====

Before installing Oracle slave, prepare a tungsten_frommysql user in Oracle identified by the same password, you are providing via "--datasource-password" property. Internal Tungsten tables are created in this user, as well as all transactions arrive here.

Below is an example of a simple (i.e. no special filtering) Oracle slave installation:

{{{
./tools/tungsten-installer --master-slave -a --cluster-hosts=localhost \
--user=oracle \
--master-host=localhost \
--home-directory=/opt/mysql2oracle/O \
--datasource-type=oracle \
--datasource-oracle-service=ORCL \
--datasource-user=tungsten \
--datasource-password=secret \
--service-name=frommysql \
--rmi-port=10000 \
--master-thl-port=12112 \
--thl-port=12110 \
--start-and-report
}}}

=== MySQL to MongoDB Replication ===

The following examples show how to install MySQL to MongoDB replication. 

==== Install a MySQL Master ====

Install a MySQL master replicator.  MySQL must use ROW replication, so you must set binlog_format=row before enabling replication.  You must enable the colnames and pkey filter so that the MongoDB can generate column names and also generate MongoDB indexes on primary key values. 

{{{
tools/tungsten-installer --master-slave -a \
  --datasource-type=mysql \
  --master-host=logos1  \
  --datasource-user=tungsten  \
  --datasource-password=secret  \
  --service-name=mongodb \
  --home-directory=/opt/continuent \
  --cluster-hosts=logos1 \
  --mysql-use-bytes-for-string=false \
  --svc-extractor-filters=colnames,pkey \
  --svc-parallelization-type=disk --start-and-report
}}}

==== Install a MongoDB Slave ====

Install MongoDB 1.8.3 and then install a MongoDB slave.  Other MongoDB versions may work as well. 

{{{
tools/tungsten-installer --master-slave -a \
  --datasource-type=mongodb \
  --master-host=logos1  \
  --datasource-user=tungsten  \
  --datasource-password=secret  \
  --service-name=mongodb \
  --home-directory=/opt/continuent \
  --cluster-hosts=logos2 \
  --skip-validation-check=InstallerMasterSlaveCheck \
  --svc-parallelization-type=disk --start-and-report
}}}

MySQL tables will be materialized as collections in MongoDB.  Columns convert to BSON properties.  SQL primary keys will be converted to (composite) indexes on collections. 

== Administration ==

=== Check replication services ===

The tool that helps you look inside the replicator is called {{{trepctl}}}, and it's located under $TUNGSTEN_HOME/tungsten/tungsten-replicator/bin.

To get a list of the existing services, run:

{{{
TUNGSTEN_BIN=$TUNGSTEN_HOME/tungsten/tungsten-replicator/bin

$TUNGSTEN_BIN/trepctl services
}}}


=== Check replication status ===

The 'status' option of trepctl will give you detailed information about the health of the replicator.
{{{
$TUNGSTEN_BIN/trepctl -service service_name status
}}}


=== Suspend and resume replication ===

To suspend a replicator service, put it offline:

{{{
$TUNGSTEN_BIN/trepctl -service service_name offline
}}}

Notice that this operation will not affect other services, if your replicator is running more than one.

To put it back online, you change the keyword.

{{{
$TUNGSTEN_BIN/trepctl -service service_name online
}}}

You may go online with several options, such as skipping one or more transactions, or replicating from a different file/position.

==== Resuming replication and skipping a transaction ====

{{{
$TUNGSTEN_BIN/trepctl -service service_name online -skip-seqno 2340
}}}

=== Inspect Transaction History Logs ===

The THL (Transaction History Logs) is the data that Tungsten has taken from the master's binary logs and transported to its servers, with the addition of some metadata.

There is a tool, called 'thl', which allows you to see the status of the logs and eventually list its contents. 

{{{
$TUNGSTEN_BIN/thl -service service_name info
}}}
This command will give you a summary of what you may find in the THL.

{{{
$TUNGSTEN_BIN/thl -service service_name index
}}}
Similar to the previous one, it will give you a list of the available THL files, with the minimum and maximum transaction number for each file.

{{{
$TUNGSTEN_BIN/thl -service service_name list
}}}
This command will list all the transactions that are available for the given service. This is potentially dangerous, since it may list millions of transactions on your screen. See the next ones for more options.

{{{
$TUNGSTEN_BIN/thl -service service_name list -low 0 -high 1858
$TUNGSTEN_BIN/thl -service service_name list -seqno 34902
}}}
The first command lists transactions in a given range, while the second lists only one specific transaction.

=== Check parallel replication status ===

==== Shards ====

{{{
$TUNGSTEN_BIN/trepctl -service service_name status -name shards
}}}

==== Tasks ====

{{{
$TUNGSTEN_BIN/trepctl -service service_name status -name tasks
}}}

==== Stores ====

{{{
$TUNGSTEN_BIN/trepctl -service service_name status -name stores
}}}

=== Resetting the Transaction History Logs ===

The THL can be cleared in order to reset the replication service and start from a clean slate.

 * Suspend replication
{{{
$TUNGSTEN_BIN/trepctl -service service_name offline
}}}
 * Remove all existing THL files
{{{
rm $TUNGSTEN_HOME/thl/service_name/*
}}}
 * Remove all existing relay files
{{{
rm $TUNGSTEN_HOME/relay/service_name/*
}}}
 * Reset the replication service schema
{{{
TRUNCATE TABLE tungsten_service_name.trep_commit_seqno;
TRUNCATE TABLE tungsten_service_name.heartbeat;
}}}
 * Resume replication
{{{
$TUNGSTEN_BIN/trepctl -service service_name online
}}}

=== Upgrading from Tungsten Replicator 2.0.4 to 2.0.5 ===

Tungsten Replicator is released quite frequently. We add planned features and fix bugs continuously. Therefore, getting [http://s3.amazonaws.com/files.continuent.com/builds/nightly/tungsten-2.0-snapshots/index.html the latest release] is very important.

If you have installed Tungsten Replicator 2.0.4 or later, and you want to upgrade to the latest version, here's what to do.

After you [http://code.google.com/p/tungsten-replicator/w/edit.do#Get_the_replicator get the replicator], run these commands for each host:

{{{
ssh host_name $TUNGSTEN_HOME/tungsten/tungsten-replicator/bin/replicator stop
./tools/update --host=host_name \
   --user=$USER \
   --release-directory=$TUNGSTEN_HOME
 
$TUNGSTEN_HOME/tungsten/tungsten-replicator/bin/trepctl -host host_name services
}}}

For example, if you have installed a regular master slave cluster using [http://code.google.com/p/tungsten-replicator/w/edit.do#Install_a_master_/_slave_cluster this recipe], the release directory will be $HOME/replication.

If the replicator is offline after the update command (that depends on your settings), you can simply put it online using trepctl.


*Notes:* 
  * Upgrading from previous version of Tungsten Replicator requires a manual re-installation, because of the new installer format.
Upgrading to the next version of Tungsten should be handled in the same way described in this section.
  * The host used in the above commands must be the same used when the server was defined. If a host can be addressed by two names (e.g _m1.mynetwork.com_ and _m1_) the same name that was used during installation must also be used for the upgrade.

=== Increasing the JVM memory size ===

The {{{--java-mem-size}}} option sets the amount of memory allocated to the JVM.  You can update the allocation on an existing replicator using the {{{tools/update}}} script.

{{{
$TUNGSTEN_HOME/tungsten/tools/update -a --java-mem-size=1024
$TUNGSTEN_HOME/tungsten/tungsten-replicator/bin/replicator restart
$TUNGSTEN_HOME/tungsten/tungsten-replicator/bin/trepctl services
}}}

=== Managing replicator log space ===

There are 3 kinds of logs used in Tungsten Replicator.
 * Master Relay Logs
 * Transaction History Logs
 * Service Logs

==== Master Relay Logs ===

The Master Relay Logs (MRL) are used when the master is extracting events from a MySQL server.  The MRL files are stored in the {{{$TUNGSTEN_HOME/relay/service_name}}} directory.  Use a symlink if you would like to store these files on a different disk.

The {{{replicator.extractor.dbms.relayLogRetention}}} value in {{{tungsten-replicator/conf/static-service_name.properties}}} defines how many MRL files are kept after the replicator service has extracted events from it.  You can configure this setting using the {{{--property}}} flag.

{{{
# New Installation
./tools/tungsten-installer ... --property=replicator.extractor.dbms.relayLogRetention=4

# Existing Installation
$TUNGSTEN_HOME/tools/configure-service -U --property=replicator.extractor.dbms.relayLogRetention=4 service_name
$TUNGSTEN_HOME/tungsten-replicator/bin/trepctl -service service_name stop
$TUNGSTEN_HOME/tungsten-replicator/bin/trepctl -service service_name start
$TUNGSTEN_HOME/tungsten-replicator/bin/trepctl -service service_name status
}}}

==== Transaction History Logs ====

The Transaction History Logs (THL) are used to store events after they are extracted, to transmit events between the master and slave and to store events prior to being applied.  The THL files are stored in the {{{$TUNGSTEN_HOME/thl/service_name}}} directory.  Use a symlink if you would like to store these files on a different disk.

The {{{replicator.store.thl.log_file_retention}}} value in {{{tungsten-replicator/conf/static-service_name.properties}}} defines how long THL files are kept.  You can configure this setting using the {{{--property}}} flag.

{{{
# New Installation
./tools/tungsten-installer ... --property=replicator.store.thl.log_file_retention=3d

# Existing Installation
$TUNGSTEN_HOME/tools/configure-service -U --property=replicator.store.thl.log_file_retention=3d service_name
$TUNGSTEN_HOME/tungsten-replicator/bin/trepctl -service service_name stop
$TUNGSTEN_HOME/tungsten-replicator/bin/trepctl -service service_name start
$TUNGSTEN_HOME/tungsten-replicator/bin/trepctl -service service_name status
}}}

==== Service Logs ====

The log files are stored in {{{tungsten-replicator/log}}}.  There are two service logs maintained by the replicator.

 * {{{user.log}}} - Partial service log that includes state changes and basic error messages
 * {{{trepsvc.log}}} - Complete service log including stack traces

The settings for these log files are located in {{{tungsten-replicator/conf/log4j.properties}}}.  You can modify the {{{log4j.appender.file.MaxFileSize}}} and {{{log4j.appender.file.MaxBackupIndex}}} values for each log file to manage how much disk space is required to hold the log files.

Check out the [TungstenReplicatorCookbook#Modify_the_configuration_template_file_prior_to_configuration] recipe for an idea of how you can update the {{{log4j.properties}}} file as part of installation.

Short link to this page: [http://bit.ly/tr20_cookbook http://bit.ly/tr20_cookbook]

== Monitoring and Troubleshooting ==

=== Monitor JVM memory usage ===

It is possible to use JConsole to monitor the memory usage of the Tungsten Replicator process.  If you see that the allocated memory is filling up, you may increase it using the {{--java-mem-size}} argument.

JConsole requires that jmxremote is enabled in {{{tungsten-replicator/conf/wrapper.conf}}}.  It is enabled by default but make sure that {{{wrapper.java.additional.3=-Dcom.sun.management.jmxremote}}} is not commented out in the file.

You can get information on JConsole at [http://java.sun.com/developer/technicalArticles/J2SE/jconsole.html].