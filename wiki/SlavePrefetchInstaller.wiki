#summary Interface design for slave pre-fetching
= Slave pre-fetch installation design =
<wiki:toc max_depth="4" />
== Overview ==
Slave pre-fetching is a feature that reads ahead of the slave applier, and warms up the pages needed by the applier by converting THL updates into selects and executing them a few seconds before the slave applies the change. This way, the page is already in memory by the time the change is applied.

== Implementation ==
The pre-fetcher is a replicator service that can be installed in two ways: either as a service to an already existing replicator or as a separate replicator. This second one, while likely less efficient, is possibly more common, as it can be used to boost a Tungsten Enterprise slave or (eventually) to speed up a MySQL native replication slave by reading the relay logs and applying from them.
Either way, the pre-fetching replicator service will not write anywhere in the database, will not have its own service database, and will borrow connecting credentials from another replication service.

 * Connection parameters will be taken from the slave that needs to be boosted. It should be on the same host, for efficiency reasons;
 * There should be a new role, *prefetch* that will set the defaults for this case.
 *  The pipeline will be a simple *thl-to-q,q-to-dbms*.
 * the THL directory must be the same as the slave being boosted;
 * the  operational mode for the THL must be read-only
 * There are three properties that tune the service, related to
   * time to read ahead
   * sleep time (purpose required)
   * events to skip

The snippet below is an example of the changes made to the static properties file.  It shows how to define a prefetch pipeline, configure, the prefetch applier, and configure the supplemental prefetch filter, which controls the rate at which transactions are fed to the parallel queue in the second stage.  

{{{
#
# Connection parameters are the ones of the slave being boosted.
#
replicator.global.db.host=127.0.0.1
replicator.global.db.port=3306
replicator.global.db.user=tungsten
replicator.global.db.password=secret

replicator.global.extract.db.host=127.0.0.1
replicator.global.extract.db.port=3306
replicator.global.extract.db.user=tungsten
replicator.global.extract.db.password=secret

#
# Define a prefetch pipeline as follows. 
#
replicator.role=prefetch

replicator.pipelines=prefetch
replicator.pipeline.prefetch=thl-to-q,q-to-dbms
replicator.pipeline.slave.stores=thl,parallel-queue

replicator.stage.thl-to-q=com.continuent.tungsten.replicator.pipeline.SingleThreadStageTask
replicator.stage.thl-to-q.extractor=thl-extractor
replicator.stage.thl-to-q.applier=parallel-q-applier
replicator.stage.thl-to-q.blockCommitRowCount=${replicator.global.buffer.size}
replicator.stage.thl-to-q.filters=prefetch

replicator.stage.q-to-dbms=com.continuent.tungsten.replicator.pipeline.SingleThreadStageTask
replicator.stage.q-to-dbms.extractor=parallel-q-extractor
replicator.stage.q-to-dbms.applier=prefetch
replicator.stage.q-to-dbms.filters=mysqlsessions,pkey,bidiSlave
replicator.stage.q-to-dbms.taskCount=${replicator.global.apply.channels}
replicator.stage.q-to-dbms.blockCommitRowCount=${replicator.global.buffer.size}

# 
# Define the THL location and set read-only flag as follows. 
#
replicator.store.thl.log_dir=/same/as/the/boosted/slave
replicator.store.thl.readOnly=true

#
# Define the prefetch applier as follows. 
#
replicator.applier.prefetch=com.continuent.tungsten.replicator.applier.JdbcPrefetcher
replicator.applier.prefetch.url=jdbc:mysql:thin://${replicator.global.db.host}:${replicator.global.db.port}/
replicator.applier.prefetch.user=${replicator.global.db.user}
replicator.applier.prefetch.password=${replicator.global.db.password}

# 
# Define the prefetch filter as follows. 
#
replicator.filter.prefetch=com.continuent.tungsten.replicator.filter.PrefetchFilter
replicator.filter.prefetch.url=jdbc:mysql:thin://${replicator.global.db.host}:${replicator.global.db.port}/
replicator.filter.prefetch.aheadMaxTime=5000
replicator.filter.prefetch.sleepTime=200
replicator.filter.prefetch.warmUpEventCount=200
}}}

All other property file values remain the same. 

== User interface ==

The new feature requires the ability of installing three different topologies of pre-fetcher

 * stand-alone replicator reading from THL
 * stand-alone replicator reading from relay logs
 * additional service of existing replicator

=== Common properties ===
In all the configurations, users can tune the pre-fetching service with the following options:

{{{
   --prefetch-enabled
   --time-ahead
   --sleep-time
   --warmup-event-count

# The above options map to the following properties:
#replicator.applier.prefetch.aheadMaxTime=5000
#replicator.applier.prefetch.sleepTime=200 
#replicator.applier.prefetch.warmUpEventCount=200
}}}

=== stand-alone replicator reading from THL on a remote server ===

{{{
./tools/tungsten-installer 
    --masterslave    # This is the role.
    --prefetch-enabled=true   # Tell it to use the prefetch applier
    # all the following options refer to the
    # slave being boosted
    --cluster-hosts
    --datasource-port
    --datasource-user
    --datasource-password
    --master-host
    --master-thl-port
    --thl-port    
}}}

=== stand-alone replicator reading from relay logs ===

{{{
./tools/tungsten-installer
    --direct
    --prefetch-enabled=true
    --master-host
    --master-port
    --master-user
    --master-password
   --slave-host
   --slave-port
   --slave-user
   --slave-password
}}}

=== additional service of existing replicator ===

{{{
./tools/configure --home-directory=/opt/continuent
/opt/continuent/tools/configure-service
    -C
    --role=slave
    --prefetch-enabled=true
    --local-service-name
    --service-type=remote
    --thl-directory
}}}